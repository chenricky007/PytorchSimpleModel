{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "987a6af6-307e-4c51-806f-eed931cb8dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\pytorch_py36\\lib\\site-packages\\numpy\\__init__.py:138: UserWarning: mkl-service package failed to import, therefore Intel(R) MKL initialization ensuring its correct out-of-the box operation under condition when Gnu OpenMP had already been loaded by Python process is not assured. Please install mkl-service package, see http://github.com/IntelPython/mkl-service\n",
      "  from . import _distributor_init\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "CUDA = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "# 需要將輸入轉換為tensor,需加入transforms.ToTensor()\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "                  # transforms.ColorJitter(brightness=(0, 2), contrast=(0, 2), saturation=(0, 2), hue=(-0.1, 0.1)),\n",
    "                  transforms.RandomResizedCrop(size = (256,256),scale=(0.7, 1.0), ratio=(1.0, 1.0)),\n",
    "                  # transforms.RandomVerticalFlip(p = 0.5),\n",
    "                  transforms.RandomHorizontalFlip(p = 0.5),\n",
    "                  transforms.Resize((256, 256)),\n",
    "                  transforms.ToTensor(),\n",
    "                  transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "                  transforms.Resize((256, 256)),\n",
    "                  transforms.ToTensor(),\n",
    "                  transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 使用 torchvision.datasets.ImageFolder 讀取訓練資料\n",
    "image_folder = ImageFolder('D:/project/dataset/AnimalFaces/afhq/train', transform = train_transform, target_transform=None)\n",
    "# 建立 DataLoader，shuffle 為 True 表示會將資料進行打亂\n",
    "train_loader = DataLoader(dataset = image_folder, batch_size = batch_size, shuffle = True, num_workers = 2)\n",
    "\n",
    "# 使用 torchvision.datasets.ImageFolder 讀取測試資料\n",
    "val_image_folder = ImageFolder('D:/project/dataset/AnimalFaces/afhq/val', transform = val_transform, target_transform=None)\n",
    "# 建立 DataLoader，shuffle 為 False 表示不會將資料進行打亂\n",
    "val_loader = DataLoader(dataset = val_image_folder, batch_size = batch_size, shuffle = False, num_workers = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ba7ec26-be37-49f8-b7e3-1b0d2fdc1e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=4, padding=0, stride=4)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "        self.fc1 = nn.Linear(in_features=256, out_features=3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = F.max_pool2d(x, kernel_size=3, padding=1, stride=2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.bn3(x)\n",
    "        x = F.max_pool2d(x, kernel_size=3, padding=1, stride=2)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.bn4(x)\n",
    "        x = F.max_pool2d(x, kernel_size=x.size()[2:])\n",
    "        # print(x.shape)\n",
    "        # x = x.view(-1, 16*5*5)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "model = Model()\n",
    "# 若報錯 Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same\n",
    "# 需要將model轉換為使用GPU\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5855a082-832e-4735-91b2-8b58ee454ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db6a37b1-574f-4d6c-9467-dbabc8ad1316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch: 0 loss 2.126123425234919 acc 0.5467391304347826\n",
      "validation: loss 0.9444607086479664 acc 0.6419270833333334\n",
      "train epoch: 1 loss 0.7163001516590948 acc 0.7254076086956521\n",
      "validation: loss 0.4271836169064045 acc 0.8157552083333334\n",
      "train epoch: 2 loss 0.5264607277901276 acc 0.8094429347826086\n",
      "validation: loss 0.2789120015998681 acc 0.8815104166666666\n",
      "train epoch: 3 loss 0.37936864741470505 acc 0.8614809782608696\n",
      "validation: loss 0.2576113014171521 acc 0.8880208333333334\n",
      "train epoch: 4 loss 0.2930524084231128 acc 0.8879076086956522\n",
      "validation: loss 0.2492869794368744 acc 0.8919270833333334\n",
      "train epoch: 5 loss 0.25000106070352635 acc 0.9099184782608696\n",
      "validation: loss 0.16496779335041842 acc 0.9212239583333334\n",
      "train epoch: 6 loss 0.22902354546215223 acc 0.9143342391304348\n",
      "validation: loss 0.23629590993126234 acc 0.9029947916666666\n",
      "train epoch: 7 loss 0.22085490634907848 acc 0.918070652173913\n",
      "validation: loss 0.245336273064216 acc 0.89453125\n",
      "train epoch: 8 loss 0.19625672069580657 acc 0.9279211956521739\n",
      "validation: loss 0.20295104710385203 acc 0.91015625\n",
      "train epoch: 9 loss 0.15427552095573882 acc 0.9427309782608696\n",
      "validation: loss 0.13992846136291823 acc 0.9361979166666666\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    train_step_count = 0.0\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # data, target = Variable(data), Variable(target)\n",
    "        \n",
    "        if CUDA:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            \n",
    "        # data = data.to(device)\n",
    "        # target = target.to(device)\n",
    "\n",
    "        # clear gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward propagation\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Calculate gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        predicted = torch.max(output.data, 1)[1]\n",
    "        train_loss += loss.item()\n",
    "        train_acc += torch.sum(predicted == target, dtype = torch.float32).item()/batch_size\n",
    "        train_step_count += 1.0\n",
    "        \n",
    "    print('train epoch:', epoch, 'loss', train_loss/train_step_count, 'acc', train_acc/train_step_count)\n",
    "            \n",
    "    val_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    val_step_count = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for val_batch_idx, (val_data, val_target) in enumerate(val_loader):\n",
    "            if CUDA:\n",
    "                val_data, val_target = val_data.cuda(), val_target.cuda()\n",
    "            val_output = model(val_data)\n",
    "            val_predicted = torch.max(val_output.data, 1)[1]\n",
    "            val_loss += criterion(val_output, val_target).item()\n",
    "            val_acc += torch.sum(val_predicted == val_target, dtype = torch.float32).item()/batch_size\n",
    "            val_step_count += 1.0\n",
    "        \n",
    "    print('validation:', 'loss', val_loss/val_step_count, 'acc', val_acc/val_step_count)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97ee4806-e155-4557-8de8-db3e5ff09640",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_weights.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0186527-ac36-4cec-89c6-aea9b4b2fcd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: acc 0.9617527173913043\n",
      "validation: acc 0.9361979166666666\n"
     ]
    }
   ],
   "source": [
    "# 讀取權重\n",
    "model.load_state_dict(torch.load('model_weights.pth'))\n",
    "\n",
    "train_acc = 0.0\n",
    "train_step_count = 0.0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for train_batch_idx, (train_data, train_target) in enumerate(train_loader):\n",
    "        if CUDA:\n",
    "            train_data, train_target = train_data.cuda(), train_target.cuda()\n",
    "        train_output = model(train_data)\n",
    "        train_predicted = torch.max(train_output.data, 1)[1]\n",
    "        train_acc += torch.sum(train_predicted == train_target, dtype = torch.float32).item()/batch_size\n",
    "        train_step_count += 1.0\n",
    "\n",
    "print('train:', 'acc', train_acc/train_step_count)\n",
    "\n",
    "val_acc = 0.0\n",
    "val_step_count = 0.0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for val_batch_idx, (val_data, val_target) in enumerate(val_loader):\n",
    "        if CUDA:\n",
    "            val_data, val_target = val_data.cuda(), val_target.cuda()\n",
    "        val_output = model(val_data)\n",
    "        val_predicted = torch.max(val_output.data, 1)[1]\n",
    "        val_acc += torch.sum(val_predicted == val_target, dtype = torch.float32).item()/batch_size\n",
    "        val_step_count += 1.0\n",
    "\n",
    "print('validation:', 'acc', val_acc/val_step_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e7a00f8-dacf-45fd-9640-50f05431c22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: acc 0.9824728260869565\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe9a391b-08fe-475c-9057-c8c1a1e1f986",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\pytorch_py36\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Model. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "# 儲存整個模型\n",
    "torch.save(model, 'model.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_py36",
   "language": "python",
   "name": "pytorch_py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
